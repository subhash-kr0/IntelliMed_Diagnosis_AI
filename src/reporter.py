import os
import pandas as pd
from datetime import datetime

def generate_markdown_report(
    report_data: dict, 
    output_dir: str = "reports", 
    filename_prefix: str = "diagnosis_report"
):
    """Generates a Markdown report summarizing the diagnosis process."""
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = os.path.join(output_dir, f"{filename_prefix}_{timestamp}.md")

    report_content = f"# Automated Disease Diagnosis Report\n"
    report_content += f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

    # --- Data Summary ---
    report_content += "## 1. Data Summary\n"
    if 'data_info' in report_data:
        df_shape = report_data['data_info'].get('shape', 'N/A')
        target_col = report_data['data_info'].get('target_column', 'N/A')
        data_path = report_data['data_info'].get('data_path', 'N/A')
        report_content += f"- **Data Source:** {data_path}\n"
        report_content += f"- **Dataset Shape:** {df_shape}\n"
        report_content += f"- **Target Column:** {target_col}\n"
    else:
        report_content += "- Data summary not available.\n"
    report_content += "\n"
    
    # --- Task Identification ---
    report_content += "## 2. Task Identification\n"
    task_type = report_data.get('task_type', 'N/A')
    report_content += f"- **Identified Task Type:** {task_type}\n\n"

    # --- EDA ---
    report_content += "## 3. Exploratory Data Analysis (EDA)\n"
    eda_path = report_data.get('eda_report_path', 'N/A')
    if eda_path and os.path.exists(eda_path):
        report_content += f"- An HTML EDA report has been generated by `pandas-profiling`.\n"
        report_content += f"- **EDA Report Location:** [{os.path.basename(eda_path)}]({os.path.relpath(eda_path, output_dir)})\n" # Relative path for md link
    else:
        report_content += "- EDA report was not generated or path is missing.\n"
    report_content += "\n"

    # --- Feature Engineering ---
    report_content += "## 4. Feature Engineering\n"
    fe_summary = report_data.get('feature_engineering_summary', {})
    if fe_summary:
        report_content += f"- **Numerical Features Processed:** {fe_summary.get('num_features_count', 'N/A')}\n"
        report_content += f"  - Imputation Strategy: {fe_summary.get('num_imputer_strategy', 'median')}\n"
        report_content += f"  - Scaling Strategy: {fe_summary.get('scaler', 'StandardScaler')}\n"
        report_content += f"- **Categorical Features Processed:** {fe_summary.get('cat_features_count', 'N/A')}\n"
        report_content += f"  - Imputation Strategy: {fe_summary.get('cat_imputer_strategy', 'most_frequent')}\n"
        report_content += f"  - Encoding Strategy: {fe_summary.get('encoder', 'OneHotEncoder')}\n"
        report_content += f"- **Processed Training Data Shape:** {fe_summary.get('X_train_shape', 'N/A')}\n"
        report_content += f"- **Processed Test Data Shape:** {fe_summary.get('X_test_shape', 'N/A')}\n"
        if fe_summary.get('label_encoder_classes'):
            report_content += f"- **Target Label Encoding Classes:** {', '.join(map(str, fe_summary['label_encoder_classes']))}\n"

    else:
        report_content += "- Feature engineering summary not available.\n"
    report_content += "\n"

    # --- Model Training and Evaluation ---
    report_content += "## 5. Model Training and Evaluation\n"
    model_results = report_data.get('model_training_results', [])
    if model_results:
        report_content += "| Model Name                 | Test F1-Score | Test ROC AUC | Test Accuracy | Test Precision | Test Recall | Best Params                                  |\n"
        report_content += "|----------------------------|---------------|--------------|---------------|----------------|-------------|----------------------------------------------|\n"
        for res in model_results:
            if "error" in res:
                report_content += f"| {res['model_name']:<26} | Error: {res['error']} (Skipped further metrics) |\n"
                continue

            f1 = res.get('test_f1_score', 'N/A')
            roc_auc = res.get('test_roc_auc', 'N/A')
            acc = res.get('test_accuracy', 'N/A')
            prec = res.get('test_precision', 'N/A')
            rec = res.get('test_recall', 'N/A')
            params_str = str(res.get('best_params', {}))
            if isinstance(f1, float): f1 = f"{f1:.4f}"
            if isinstance(roc_auc, float): roc_auc = f"{roc_auc:.4f}"
            if isinstance(acc, float): acc = f"{acc:.4f}"
            if isinstance(prec, float): prec = f"{prec:.4f}"
            if isinstance(rec, float): rec = f"{rec:.4f}"

            report_content += f"| {res['model_name']:<26} | {f1:<13} | {roc_auc:<12} | {acc:<13} | {prec:<14} | {rec:<11} | {params_str[:40]:<44}{(params_str[40:] and '...') if len(params_str)>40 else ''} |\n"
    else:
        report_content += "- No model training results available.\n"
    report_content += "\n"

    # --- Best Model ---
    report_content += "## 6. Best Model Selected\n"
    best_model_name = report_data.get('best_model_name', 'N/A')
    saved_model_path = report_data.get('saved_model_path', 'N/A')
    preprocessor_path = report_data.get('saved_preprocessor_path', 'N/A')

    report_content += f"- **Best Performing Model:** {best_model_name}\n"
    if saved_model_path and os.path.exists(saved_model_path):
        report_content += f"- **Saved Model Location:** [{os.path.basename(saved_model_path)}]({os.path.relpath(saved_model_path, output_dir)})\n"
    else:
        report_content += "- Best model was not saved or path is missing.\n"
    
    if preprocessor_path and os.path.exists(preprocessor_path):
        report_content += f"- **Saved Preprocessor Location:** [{os.path.basename(preprocessor_path)}]({os.path.relpath(preprocessor_path, output_dir)})\n"
    else:
        report_content += "- Preprocessor was not saved or path is missing.\n"

    if best_model_name != 'N/A' and model_results:
        best_model_detail = next((item for item in model_results if item["model_name"] == best_model_name), None)
        if best_model_detail and "classification_report_test" in best_model_detail:
            report_content += "\n### Detailed Classification Report for Best Model (Test Set):\n"
            report_content += "```\n"
            # Format the dict report nicely
            clf_report_dict = best_model_detail["classification_report_test"]
            # Header
            report_content += f"{'':<12} {'precision':<10} {'recall':<10} {'f1-score':<10} {'support':<10}\n"
            for label, metrics in clf_report_dict.items():
                if isinstance(metrics, dict): # Class-specific metrics
                    p = f"{metrics.get('precision', 0):.2f}"
                    r = f"{metrics.get('recall', 0):.2f}"
                    f1 = f"{metrics.get('f1-score', 0):.2f}"
                    s = f"{metrics.get('support', 0):.0f}"
                    report_content += f"{label:<12} {p:<10} {r:<10} {f1:<10} {s:<10}\n"
                elif label in ['accuracy']: # Overall accuracy
                     acc_val = f"{metrics:.2f}"
                     # Get total support for alignment
                     total_support = sum(clf_report_dict[k].get('support', 0) for k in clf_report_dict if isinstance(clf_report_dict[k], dict))
                     report_content += f"\n{'accuracy':<12} {'':<10} {'':<10} {acc_val:<10} {total_support:<10.0f}\n"
                elif label in ['macro avg', 'weighted avg']: # Averages
                    p_avg = f"{metrics.get('precision', 0):.2f}"
                    r_avg = f"{metrics.get('recall', 0):.2f}"
                    f1_avg = f"{metrics.get('f1-score', 0):.2f}"
                    s_avg = f"{metrics.get('support', 0):.0f}"
                    report_content += f"{label:<12} {p_avg:<10} {r_avg:<10} {f1_avg:<10} {s_avg:<10}\n"

            report_content += "```\n"

    report_content += "\n"
    
    # --- Conclusion ---
    report_content += "## 7. Conclusion & Next Steps\n"
    report_content += "- This report summarizes the automated diagnosis process. Review the EDA and model performance for insights.\n"
    report_content += "- For real-world applications, further validation, domain expert consultation, and ethical considerations are crucial.\n"
    report_content += "- Consider exploring more complex models, advanced feature engineering, or larger hyperparameter search spaces if needed.\n"

    try:
        with open(report_filename, 'w') as f:
            f.write(report_content)
        print(f"Markdown report generated: {report_filename}")
        return report_filename
    except Exception as e:
        print(f"Error writing Markdown report: {e}")
        return None